{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8.1 \n",
    "\n",
    "Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = dense_layer(X, n_hidden1, name='hidden1')\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name='hidden2')\n",
    "    hidden3 = dense_layer(hidden2, n_hidden3, name='hidden3')\n",
    "    hidden4 = dense_layer(hidden3, n_hidden4, name='hidden4')\n",
    "    hidden5 = dense_layer(hidden4, n_hidden5, name='hidden5')\n",
    "    logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8.2\n",
    "\n",
    "Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "`Adam optimation`: adaptive moment estimation, combines the ideas of Momentum optimization and RMSProp.\n",
    "\n",
    "`Momentum optimization`: keep track of an exponentially decaying average of past gradients.\n",
    "\n",
    "`RMSProp`: keep track of an exponentially decaying of past squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_test = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(X, y, n_batches):\n",
    "    np.random.seed(seed=42)\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    for i_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch = X[i_idx]\n",
    "        y_batch = y[i_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 0.04894622 accuracy_train: 1.0 accuracy_test: 0.9820977\n",
      "1 loss 0.04120996 accuracy_train: 0.98 accuracy_test: 0.98715705\n",
      "2 loss 0.03728239 accuracy_train: 1.0 accuracy_test: 0.98813\n",
      "3 loss 0.030111544 accuracy_train: 1.0 accuracy_test: 0.9910488\n",
      "4 loss 0.026404124 accuracy_train: 1.0 accuracy_test: 0.9918272\n",
      "5 loss 0.0332601 accuracy_train: 1.0 accuracy_test: 0.99065965\n",
      "6 loss 0.030299986 accuracy_train: 1.0 accuracy_test: 0.9918272\n",
      "7 loss 0.02770311 accuracy_train: 1.0 accuracy_test: 0.99143803\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 50\n",
    "n_batches = len(X_train) // batch_size\n",
    "best_loss = float('inf')\n",
    "patience = 2\n",
    "cnt_patience = 0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_split(X_train, y_train, n_batches):\n",
    "            sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        loss_test = loss.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, 'loss', loss_test, 'accuracy_train:', accuracy_train, 'accuracy_test:', accuracy_test)\n",
    "        if loss_test < best_loss:\n",
    "            best_loss = loss_test\n",
    "        else:\n",
    "            cnt_patience += 1\n",
    "            if cnt_patience > patience:\n",
    "                'Early stopping!'\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can use `tf.keras.callbacks.EarlyStopping`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8.3\n",
    "\n",
    "Tune the hyperparameters using cross-validation and see what precision you can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order to tune hyperparameters in Neural Networks\n",
    "\n",
    "Answers from [stackoverflow](https://stackoverflow.com/questions/37467647/in-what-order-should-we-tune-hyperparameters-in-neural-networks):\n",
    "\n",
    "**My general order is:**\n",
    "\n",
    "- Batch size, as it will largely affect the training time of future experiments.\n",
    "\n",
    "- Architecture of the network:\n",
    "\n",
    "    - Number of neurons in the network\n",
    "    - Number of layers\n",
    "\n",
    "- Rest (dropout, L2 reg, etc.)\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "I'd assume that the optimal values of\n",
    "\n",
    "- learning rate and batch size\n",
    "- learning rate and number of neurons\n",
    "- number of neurons and number of layers\n",
    "\n",
    "strongly depend on each other. I am not an expert on that field though.\n",
    "\n",
    "**As for your hyperparameters:**\n",
    "\n",
    "- For the Adam optimizer: \"Recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999.\" (source)\n",
    "- For the learning rate with Adam and RMSProp, I found values around 0.001 to be optimal for most problems.\n",
    "- As an alternative to Adam, you can also use RMSProp, which reduces the memory footprint by up to 33%. See this answer for more details.\n",
    "- You could also tune the initial weight values (see All you need is a good init). Although, the Xavier initializer seems to be a good way to prevent having to tune the weight inits.\n",
    "- I don't tune the number of iterations / epochs as a hyperparameter. I train the net until its validation error converges. However, I give each run a time budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom estimator with scikit learn\n",
    "\n",
    "The following refers to [scikit learn documentation](http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator) and a blog article from [Daniel Hnyk](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/).\n",
    "\n",
    "**Estimator types**\n",
    "\n",
    "Some common functionality depends on the kind of estimator passed. For different tasks, you can choose:\n",
    "\n",
    "- ClassifierMixin\n",
    "- RegressorMixin\n",
    "- ClusterMixin\n",
    "\n",
    "**get_params and set_params**\n",
    "\n",
    "All estimators must have `get_params` and `set_params` functions. They are inherited from `BaseEstimator`.\n",
    "\n",
    "**Pipeline compatibility**\n",
    "\n",
    "For an estimator to be usable together with `pipeline.Pipeline`in any but the last step, it needs to provide a `fit` or `fit_transform` function. \n",
    "\n",
    "To be able to evaluate the pipeline on any data but the training set, it also needs to provide a `transform` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 batch_size=50, \n",
    "                 n_neuron=100):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_neuron = n_neuron\n",
    "        \n",
    "    def reset_graph(self, seed=42):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \n",
    "        self.reset_graph()\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "        \n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "        \n",
    "        dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "\n",
    "        with tf.name_scope('dnn'):\n",
    "            hidden1 = dense_layer(X, self.n_neuron, name='hidden1')\n",
    "            hidden2 = dense_layer(hidden1, self.n_neuron, name='hidden2')\n",
    "            hidden3 = dense_layer(hidden2, self.n_neuron, name='hidden3')\n",
    "            hidden4 = dense_layer(hidden3, self.n_neuron, name='hidden4')\n",
    "            hidden5 = dense_layer(hidden4, self.n_neuron, name='hidden5')\n",
    "            logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
    "            \n",
    "        with tf.name_scope('softmax'):\n",
    "            y_proba = tf.nn.softmax(logits, axis=1, name='y_proba')\n",
    "    \n",
    "        with tf.name_scope('loss'):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name='loss')\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            \n",
    "        with tf.name_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            \n",
    "        self._training_op = training_op\n",
    "        self._accuracy = accuracy\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._logits = logits\n",
    "        self._y_proba = y_proba\n",
    "        \n",
    "    def shuffle_split(self, X, y, n_batches):\n",
    "        np.random.seed(seed=42)\n",
    "        rnd_idx = np.random.permutation(len(X))\n",
    "        for i_idx in np.array_split(rnd_idx, n_batches):\n",
    "            X_batch = X[i_idx]\n",
    "            y_batch = y[i_idx]\n",
    "            yield X_batch, y_batch\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=5, X_valid=None, y_valid=None):\n",
    "        self.n_batches = len(X) // self.batch_size\n",
    "        \n",
    "        n_inputs = X.shape[1]\n",
    "        self.n_outputs = len(np.unique(y))\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        self._build_graph(n_inputs, self.n_outputs)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        self._session = tf.Session()\n",
    "        with self._session.as_default() as sess:\n",
    "            init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                for X_batch, y_batch in self.shuffle_split(X, y, self.n_batches):\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    sess.run([self._training_op], feed_dict=feed_dict)\n",
    "                accuracy_train = self._accuracy.eval(feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    accuracy_valid = self._accuracy.eval(feed_dict={self._X: X_valid, self._y: y_valid})\n",
    "                    print('epoch:', epoch, 'accuracy_valid:', accuracy_valid*100)\n",
    "                else:\n",
    "                    print('epoch:', epoch, 'accuracy_train:', accuracy_train*100)\n",
    "                \n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        with self._session.as_default() as sess:\n",
    "            y_proba = sess.run([self._y_proba], feed_dict={self._X: X})\n",
    "            return np.array(y_proba).reshape((len(X), self.n_outputs))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_valid = mnist.test.images\n",
    "y_valid = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 accuracy_valid: 94.73000168800354\n",
      "epoch: 1 accuracy_valid: 96.13000154495239\n",
      "epoch: 2 accuracy_valid: 96.670001745224\n",
      "epoch: 3 accuracy_valid: 96.5399980545044\n",
      "epoch: 4 accuracy_valid: 96.02000117301941\n",
      "epoch: 5 accuracy_valid: 96.42999768257141\n",
      "epoch: 6 accuracy_valid: 97.39999771118164\n",
      "epoch: 7 accuracy_valid: 97.40999937057495\n",
      "epoch: 8 accuracy_valid: 96.74999713897705\n",
      "epoch: 9 accuracy_valid: 97.49000072479248\n",
      "epoch: 10 accuracy_valid: 97.39000201225281\n",
      "epoch: 11 accuracy_valid: 97.47999906539917\n",
      "epoch: 12 accuracy_valid: 96.99000120162964\n",
      "epoch: 13 accuracy_valid: 97.24000096321106\n",
      "epoch: 14 accuracy_valid: 97.72999882698059\n",
      "epoch: 15 accuracy_valid: 97.680002450943\n",
      "epoch: 16 accuracy_valid: 97.39000201225281\n",
      "epoch: 17 accuracy_valid: 97.57999777793884\n",
      "epoch: 18 accuracy_valid: 97.39999771118164\n",
      "epoch: 19 accuracy_valid: 97.53000140190125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DnnClassifier(batch_size=50, n_neuron=100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DnnClassifier()\n",
    "dnn_clf.fit(X_train, y_train, 20, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the valid set is 97.53%, which is no bad. So let's try to tune the n_neuron and batch_size to see if it can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 accuracy_valid: 95.46999931335449\n",
      "epoch: 1 accuracy_valid: 95.56000232696533\n",
      "epoch: 2 accuracy_valid: 95.52000164985657\n",
      "epoch: 3 accuracy_valid: 96.07999920845032\n",
      "epoch: 4 accuracy_valid: 96.64999842643738\n",
      "epoch: 5 accuracy_valid: 96.32999897003174\n",
      "epoch: 6 accuracy_valid: 96.85999751091003\n",
      "epoch: 7 accuracy_valid: 96.45000100135803\n",
      "epoch: 8 accuracy_valid: 96.99000120162964\n",
      "epoch: 9 accuracy_valid: 96.61999940872192\n",
      "epoch: 10 accuracy_valid: 96.61999940872192\n",
      "epoch: 11 accuracy_valid: 96.60000205039978\n",
      "epoch: 12 accuracy_valid: 96.90999984741211\n",
      "epoch: 13 accuracy_valid: 96.96999788284302\n",
      "epoch: 14 accuracy_valid: 95.98000049591064\n",
      "epoch: 15 accuracy_valid: 96.27000093460083\n",
      "epoch: 16 accuracy_valid: 97.00999855995178\n",
      "epoch: 17 accuracy_valid: 97.21999764442444\n",
      "epoch: 18 accuracy_valid: 97.06000089645386\n",
      "epoch: 19 accuracy_valid: 96.93999886512756\n",
      "epoch: 0 accuracy_valid: 93.65000128746033\n",
      "epoch: 1 accuracy_valid: 95.63000202178955\n",
      "epoch: 2 accuracy_valid: 95.63000202178955\n",
      "epoch: 3 accuracy_valid: 95.7099974155426\n",
      "epoch: 4 accuracy_valid: 96.28999829292297\n",
      "epoch: 5 accuracy_valid: 96.53000235557556\n",
      "epoch: 6 accuracy_valid: 96.68999910354614\n",
      "epoch: 7 accuracy_valid: 96.5399980545044\n",
      "epoch: 8 accuracy_valid: 96.78000211715698\n",
      "epoch: 9 accuracy_valid: 96.89000248908997\n",
      "epoch: 10 accuracy_valid: 96.67999744415283\n",
      "epoch: 11 accuracy_valid: 97.2000002861023\n",
      "epoch: 12 accuracy_valid: 97.10999727249146\n",
      "epoch: 13 accuracy_valid: 97.00999855995178\n",
      "epoch: 14 accuracy_valid: 97.079998254776\n",
      "epoch: 15 accuracy_valid: 96.92000150680542\n",
      "epoch: 16 accuracy_valid: 97.24000096321106\n",
      "epoch: 17 accuracy_valid: 97.03999757766724\n",
      "epoch: 18 accuracy_valid: 96.92000150680542\n",
      "epoch: 19 accuracy_valid: 97.08999991416931\n",
      "epoch: 0 accuracy_valid: 93.33999752998352\n",
      "epoch: 1 accuracy_valid: 94.92999911308289\n",
      "epoch: 2 accuracy_valid: 95.45000195503235\n",
      "epoch: 3 accuracy_valid: 95.3499972820282\n",
      "epoch: 4 accuracy_valid: 96.03000283241272\n",
      "epoch: 5 accuracy_valid: 96.5499997138977\n",
      "epoch: 6 accuracy_valid: 96.41000032424927\n",
      "epoch: 7 accuracy_valid: 96.24999761581421\n",
      "epoch: 8 accuracy_valid: 95.95999717712402\n",
      "epoch: 9 accuracy_valid: 96.07999920845032\n",
      "epoch: 10 accuracy_valid: 96.77000045776367\n",
      "epoch: 11 accuracy_valid: 96.82999849319458\n",
      "epoch: 12 accuracy_valid: 97.4399983882904\n",
      "epoch: 13 accuracy_valid: 97.04999923706055\n",
      "epoch: 14 accuracy_valid: 97.079998254776\n",
      "epoch: 15 accuracy_valid: 96.56000137329102\n",
      "epoch: 16 accuracy_valid: 97.04999923706055\n",
      "epoch: 17 accuracy_valid: 97.65999913215637\n",
      "epoch: 18 accuracy_valid: 96.8500018119812\n",
      "epoch: 19 accuracy_valid: 97.13000059127808\n",
      "epoch: 0 accuracy_valid: 95.4800009727478\n",
      "epoch: 1 accuracy_valid: 95.77999711036682\n",
      "epoch: 2 accuracy_valid: 95.77000141143799\n",
      "epoch: 3 accuracy_valid: 96.0099995136261\n",
      "epoch: 4 accuracy_valid: 95.95000147819519\n",
      "epoch: 5 accuracy_valid: 96.21000289916992\n",
      "epoch: 6 accuracy_valid: 96.21000289916992\n",
      "epoch: 7 accuracy_valid: 96.49999737739563\n",
      "epoch: 8 accuracy_valid: 96.72999978065491\n",
      "epoch: 9 accuracy_valid: 96.28000259399414\n",
      "epoch: 10 accuracy_valid: 97.10000157356262\n",
      "epoch: 11 accuracy_valid: 96.78999781608582\n",
      "epoch: 12 accuracy_valid: 96.46000266075134\n",
      "epoch: 13 accuracy_valid: 96.86999917030334\n",
      "epoch: 14 accuracy_valid: 96.7199981212616\n",
      "epoch: 15 accuracy_valid: 97.2100019454956\n",
      "epoch: 16 accuracy_valid: 96.88000082969666\n",
      "epoch: 17 accuracy_valid: 97.28999733924866\n",
      "epoch: 18 accuracy_valid: 96.85999751091003\n",
      "epoch: 19 accuracy_valid: 96.64999842643738\n",
      "epoch: 0 accuracy_valid: 95.06000280380249\n",
      "epoch: 1 accuracy_valid: 95.81999778747559\n",
      "epoch: 2 accuracy_valid: 95.75999975204468\n",
      "epoch: 3 accuracy_valid: 96.17999792098999\n",
      "epoch: 4 accuracy_valid: 96.10000252723694\n",
      "epoch: 5 accuracy_valid: 96.06000185012817\n",
      "epoch: 6 accuracy_valid: 96.61999940872192\n",
      "epoch: 7 accuracy_valid: 96.34000062942505\n",
      "epoch: 8 accuracy_valid: 96.60999774932861\n",
      "epoch: 9 accuracy_valid: 96.64000272750854\n",
      "epoch: 10 accuracy_valid: 96.8999981880188\n",
      "epoch: 11 accuracy_valid: 97.28000164031982\n",
      "epoch: 12 accuracy_valid: 96.60000205039978\n",
      "epoch: 13 accuracy_valid: 96.78999781608582\n",
      "epoch: 14 accuracy_valid: 96.670001745224\n",
      "epoch: 15 accuracy_valid: 96.95000052452087\n",
      "epoch: 16 accuracy_valid: 96.72999978065491\n",
      "epoch: 17 accuracy_valid: 97.45000004768372\n",
      "epoch: 18 accuracy_valid: 97.47999906539917\n",
      "epoch: 19 accuracy_valid: 97.22999930381775\n",
      "epoch: 0 accuracy_valid: 94.47000026702881\n",
      "epoch: 1 accuracy_valid: 95.37000060081482\n",
      "epoch: 2 accuracy_valid: 95.5299973487854\n",
      "epoch: 3 accuracy_valid: 96.07999920845032\n",
      "epoch: 4 accuracy_valid: 96.43999934196472\n",
      "epoch: 5 accuracy_valid: 96.46000266075134\n",
      "epoch: 6 accuracy_valid: 96.39000296592712\n",
      "epoch: 7 accuracy_valid: 96.35000228881836\n",
      "epoch: 8 accuracy_valid: 96.63000106811523\n",
      "epoch: 9 accuracy_valid: 96.77000045776367\n",
      "epoch: 10 accuracy_valid: 97.079998254776\n",
      "epoch: 11 accuracy_valid: 97.36999869346619\n",
      "epoch: 12 accuracy_valid: 97.10999727249146\n",
      "epoch: 13 accuracy_valid: 97.2000002861023\n",
      "epoch: 14 accuracy_valid: 97.2000002861023\n",
      "epoch: 15 accuracy_valid: 97.21999764442444\n",
      "epoch: 16 accuracy_valid: 97.18999862670898\n",
      "epoch: 17 accuracy_valid: 97.32999801635742\n",
      "epoch: 18 accuracy_valid: 97.28000164031982\n",
      "epoch: 19 accuracy_valid: 97.4399983882904\n",
      "epoch: 0 accuracy_valid: 94.55000162124634\n",
      "epoch: 1 accuracy_valid: 95.53999900817871\n",
      "epoch: 2 accuracy_valid: 95.4200029373169\n",
      "epoch: 3 accuracy_valid: 95.67999839782715\n",
      "epoch: 4 accuracy_valid: 95.88000178337097\n",
      "epoch: 5 accuracy_valid: 95.95999717712402\n",
      "epoch: 6 accuracy_valid: 96.49999737739563\n",
      "epoch: 7 accuracy_valid: 96.45000100135803\n",
      "epoch: 8 accuracy_valid: 96.5399980545044\n",
      "epoch: 9 accuracy_valid: 96.42999768257141\n",
      "epoch: 10 accuracy_valid: 96.45000100135803\n",
      "epoch: 11 accuracy_valid: 96.68999910354614\n",
      "epoch: 12 accuracy_valid: 96.34000062942505\n",
      "epoch: 13 accuracy_valid: 96.5399980545044\n",
      "epoch: 14 accuracy_valid: 96.39999866485596\n",
      "epoch: 15 accuracy_valid: 96.48000001907349\n",
      "epoch: 16 accuracy_valid: 96.67999744415283\n",
      "epoch: 17 accuracy_valid: 96.68999910354614\n",
      "epoch: 18 accuracy_valid: 96.46999835968018\n",
      "epoch: 19 accuracy_valid: 96.29999995231628\n",
      "epoch: 0 accuracy_valid: 94.34000253677368\n",
      "epoch: 1 accuracy_valid: 95.169997215271\n",
      "epoch: 2 accuracy_valid: 95.59000134468079\n",
      "epoch: 3 accuracy_valid: 95.49000263214111\n",
      "epoch: 4 accuracy_valid: 95.4800009727478\n",
      "epoch: 5 accuracy_valid: 95.42999863624573\n",
      "epoch: 6 accuracy_valid: 95.92999815940857\n",
      "epoch: 7 accuracy_valid: 96.02000117301941\n",
      "epoch: 8 accuracy_valid: 95.89999914169312\n",
      "epoch: 9 accuracy_valid: 95.92999815940857\n",
      "epoch: 10 accuracy_valid: 96.5399980545044\n",
      "epoch: 11 accuracy_valid: 95.85000276565552\n",
      "epoch: 12 accuracy_valid: 95.63999772071838\n",
      "epoch: 13 accuracy_valid: 96.68999910354614\n",
      "epoch: 14 accuracy_valid: 96.31999731063843\n",
      "epoch: 15 accuracy_valid: 96.21999859809875\n",
      "epoch: 16 accuracy_valid: 96.60000205039978\n",
      "epoch: 17 accuracy_valid: 96.11999988555908\n",
      "epoch: 18 accuracy_valid: 96.82999849319458\n",
      "epoch: 19 accuracy_valid: 96.67999744415283\n",
      "epoch: 0 accuracy_valid: 94.41999793052673\n",
      "epoch: 1 accuracy_valid: 95.28999924659729\n",
      "epoch: 2 accuracy_valid: 95.63999772071838\n",
      "epoch: 3 accuracy_valid: 95.81000208854675\n",
      "epoch: 4 accuracy_valid: 96.1899995803833\n",
      "epoch: 5 accuracy_valid: 96.16000056266785\n",
      "epoch: 6 accuracy_valid: 96.60999774932861\n",
      "epoch: 7 accuracy_valid: 96.57999873161316\n",
      "epoch: 8 accuracy_valid: 96.81000113487244\n",
      "epoch: 9 accuracy_valid: 96.8999981880188\n",
      "epoch: 10 accuracy_valid: 96.82999849319458\n",
      "epoch: 11 accuracy_valid: 96.81000113487244\n",
      "epoch: 12 accuracy_valid: 96.53000235557556\n",
      "epoch: 13 accuracy_valid: 96.45000100135803\n",
      "epoch: 14 accuracy_valid: 96.86999917030334\n",
      "epoch: 15 accuracy_valid: 96.48000001907349\n",
      "epoch: 16 accuracy_valid: 97.11999893188477\n",
      "epoch: 17 accuracy_valid: 96.71000242233276\n",
      "epoch: 18 accuracy_valid: 96.74999713897705\n",
      "epoch: 19 accuracy_valid: 96.70000076293945\n",
      "epoch: 0 accuracy_valid: 95.4800009727478\n",
      "epoch: 1 accuracy_valid: 95.77999711036682\n",
      "epoch: 2 accuracy_valid: 95.77000141143799\n",
      "epoch: 3 accuracy_valid: 96.0099995136261\n",
      "epoch: 4 accuracy_valid: 95.95000147819519\n",
      "epoch: 5 accuracy_valid: 96.21000289916992\n",
      "epoch: 6 accuracy_valid: 96.21000289916992\n",
      "epoch: 7 accuracy_valid: 96.49999737739563\n",
      "epoch: 8 accuracy_valid: 96.72999978065491\n",
      "epoch: 9 accuracy_valid: 96.28000259399414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 accuracy_valid: 97.10000157356262\n",
      "epoch: 11 accuracy_valid: 96.78999781608582\n",
      "epoch: 12 accuracy_valid: 96.46000266075134\n",
      "epoch: 13 accuracy_valid: 96.86999917030334\n",
      "epoch: 14 accuracy_valid: 96.7199981212616\n",
      "epoch: 15 accuracy_valid: 97.2100019454956\n",
      "epoch: 16 accuracy_valid: 96.88000082969666\n",
      "epoch: 17 accuracy_valid: 97.28999733924866\n",
      "epoch: 18 accuracy_valid: 96.85999751091003\n",
      "epoch: 19 accuracy_valid: 96.64999842643738\n",
      "epoch: 0 accuracy_valid: 95.06000280380249\n",
      "epoch: 1 accuracy_valid: 95.81999778747559\n",
      "epoch: 2 accuracy_valid: 95.75999975204468\n",
      "epoch: 3 accuracy_valid: 96.17999792098999\n",
      "epoch: 4 accuracy_valid: 96.10000252723694\n",
      "epoch: 5 accuracy_valid: 96.06000185012817\n",
      "epoch: 6 accuracy_valid: 96.61999940872192\n",
      "epoch: 7 accuracy_valid: 96.34000062942505\n",
      "epoch: 8 accuracy_valid: 96.60999774932861\n",
      "epoch: 9 accuracy_valid: 96.64000272750854\n",
      "epoch: 10 accuracy_valid: 96.8999981880188\n",
      "epoch: 11 accuracy_valid: 97.28000164031982\n",
      "epoch: 12 accuracy_valid: 96.60000205039978\n",
      "epoch: 13 accuracy_valid: 96.78999781608582\n",
      "epoch: 14 accuracy_valid: 96.670001745224\n",
      "epoch: 15 accuracy_valid: 96.95000052452087\n",
      "epoch: 16 accuracy_valid: 96.72999978065491\n",
      "epoch: 17 accuracy_valid: 97.45000004768372\n",
      "epoch: 18 accuracy_valid: 97.47999906539917\n",
      "epoch: 19 accuracy_valid: 97.22999930381775\n",
      "epoch: 0 accuracy_valid: 94.47000026702881\n",
      "epoch: 1 accuracy_valid: 95.37000060081482\n",
      "epoch: 2 accuracy_valid: 95.5299973487854\n",
      "epoch: 3 accuracy_valid: 96.07999920845032\n",
      "epoch: 4 accuracy_valid: 96.43999934196472\n",
      "epoch: 5 accuracy_valid: 96.46000266075134\n",
      "epoch: 6 accuracy_valid: 96.39000296592712\n",
      "epoch: 7 accuracy_valid: 96.35000228881836\n",
      "epoch: 8 accuracy_valid: 96.63000106811523\n",
      "epoch: 9 accuracy_valid: 96.77000045776367\n",
      "epoch: 10 accuracy_valid: 97.079998254776\n",
      "epoch: 11 accuracy_valid: 97.36999869346619\n",
      "epoch: 12 accuracy_valid: 97.10999727249146\n",
      "epoch: 13 accuracy_valid: 97.2000002861023\n",
      "epoch: 14 accuracy_valid: 97.2000002861023\n",
      "epoch: 15 accuracy_valid: 97.21999764442444\n",
      "epoch: 16 accuracy_valid: 97.18999862670898\n",
      "epoch: 17 accuracy_valid: 97.32999801635742\n",
      "epoch: 18 accuracy_valid: 97.28000164031982\n",
      "epoch: 19 accuracy_valid: 97.4399983882904\n",
      "epoch: 0 accuracy_valid: 94.73000168800354\n",
      "epoch: 1 accuracy_valid: 96.13000154495239\n",
      "epoch: 2 accuracy_valid: 96.670001745224\n",
      "epoch: 3 accuracy_valid: 96.5399980545044\n",
      "epoch: 4 accuracy_valid: 96.02000117301941\n",
      "epoch: 5 accuracy_valid: 96.42999768257141\n",
      "epoch: 6 accuracy_valid: 97.39999771118164\n",
      "epoch: 7 accuracy_valid: 97.40999937057495\n",
      "epoch: 8 accuracy_valid: 96.74999713897705\n",
      "epoch: 9 accuracy_valid: 97.49000072479248\n",
      "epoch: 10 accuracy_valid: 97.39000201225281\n",
      "epoch: 11 accuracy_valid: 97.47999906539917\n",
      "epoch: 12 accuracy_valid: 96.99000120162964\n",
      "epoch: 13 accuracy_valid: 97.24000096321106\n",
      "epoch: 14 accuracy_valid: 97.72999882698059\n",
      "epoch: 15 accuracy_valid: 97.680002450943\n",
      "epoch: 16 accuracy_valid: 97.39000201225281\n",
      "epoch: 17 accuracy_valid: 97.57999777793884\n",
      "epoch: 18 accuracy_valid: 97.39999771118164\n",
      "epoch: 19 accuracy_valid: 97.53000140190125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "params_grid = [{'batch_size': [20, 50]}, {'n_neuron': [50, 100]}]\n",
    "gs = GridSearchCV(DnnClassifier(), params_grid, \n",
    "                 fit_params={'n_epochs': 20, 'X_valid': X_valid, 'y_valid': y_valid})\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_valid_pred = gs.predict(X_valid)\n",
    "print(accuracy_score(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is not improved after tuning. It might be because that the `fit` method doesn't implement early stopping yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
