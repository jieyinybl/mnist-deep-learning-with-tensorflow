{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8.1 \n",
    "\n",
    "Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = dense_layer(X, n_hidden1, name='hidden1')\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name='hidden2')\n",
    "    hidden3 = dense_layer(hidden2, n_hidden3, name='hidden3')\n",
    "    hidden4 = dense_layer(hidden3, n_hidden4, name='hidden4')\n",
    "    hidden5 = dense_layer(hidden4, n_hidden5, name='hidden5')\n",
    "    logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8.2\n",
    "\n",
    "Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "`Adam optimation`: adaptive moment estimation, combines the ideas of Momentum optimization and RMSProp.\n",
    "\n",
    "`Momentum optimization`: keep track of an exponentially decaying average of past gradients.\n",
    "\n",
    "`RMSProp`: keep track of an exponentially decaying of past squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_test = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(X, y, n_batches):\n",
    "    np.random.seed(seed=42)\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    for i_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch = X[i_idx]\n",
    "        y_batch = y[i_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 50\n",
    "n_batches = len(X_train) // batch_size\n",
    "best_loss = float('inf')\n",
    "patience = 2\n",
    "cnt_patience = 0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_split(X_train, y_train, n_batches):\n",
    "            sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        loss_test = loss.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, 'loss', loss_test, 'accuracy_train:', accuracy_train, 'accuracy_test:', accuracy_test)\n",
    "        if loss_test < best_loss:\n",
    "            best_loss = loss_test\n",
    "        else:\n",
    "            cnt_patience += 1\n",
    "            if cnt_patience > patience:\n",
    "                'Early stopping!'\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can use `tf.keras.callbacks.EarlyStopping`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8.3\n",
    "\n",
    "Tune the hyperparameters using cross-validation and see what precision you can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order to tune hyperparameters in Neural Networks\n",
    "\n",
    "Answers from [stackoverflow](https://stackoverflow.com/questions/37467647/in-what-order-should-we-tune-hyperparameters-in-neural-networks):\n",
    "\n",
    "**My general order is:**\n",
    "\n",
    "- Batch size, as it will largely affect the training time of future experiments.\n",
    "\n",
    "- Architecture of the network:\n",
    "\n",
    "    - Number of neurons in the network\n",
    "    - Number of layers\n",
    "\n",
    "- Rest (dropout, L2 reg, etc.)\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "I'd assume that the optimal values of\n",
    "\n",
    "- learning rate and batch size\n",
    "- learning rate and number of neurons\n",
    "- number of neurons and number of layers\n",
    "\n",
    "strongly depend on each other. I am not an expert on that field though.\n",
    "\n",
    "**As for your hyperparameters:**\n",
    "\n",
    "- For the Adam optimizer: \"Recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999.\" (source)\n",
    "- For the learning rate with Adam and RMSProp, I found values around 0.001 to be optimal for most problems.\n",
    "- As an alternative to Adam, you can also use RMSProp, which reduces the memory footprint by up to 33%. See this answer for more details.\n",
    "- You could also tune the initial weight values (see All you need is a good init). Although, the Xavier initializer seems to be a good way to prevent having to tune the weight inits.\n",
    "- I don't tune the number of iterations / epochs as a hyperparameter. I train the net until its validation error converges. However, I give each run a time budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom estimator with scikit learn\n",
    "\n",
    "The following refers to [scikit learn documentation](http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator) and a blog article from [Daniel Hnyk](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/).\n",
    "\n",
    "**Estimator types**\n",
    "\n",
    "Some common functionality depends on the kind of estimator passed. For different tasks, you can choose:\n",
    "\n",
    "- ClassifierMixin\n",
    "- RegressorMixin\n",
    "- ClusterMixin\n",
    "\n",
    "**get_params and set_params**\n",
    "\n",
    "All estimators must have `get_params` and `set_params` functions. They are inherited from `BaseEstimator`.\n",
    "\n",
    "**Pipeline compatibility**\n",
    "\n",
    "For an estimator to be usable together with `pipeline.Pipeline`in any but the last step, it needs to provide a `fit` or `fit_transform` function. \n",
    "\n",
    "To be able to evaluate the pipeline on any data but the training set, it also needs to provide a `transform` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 batch_size=50, \n",
    "                 n_neuron=100):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_neuron = n_neuron\n",
    "        \n",
    "    def reset_graph(self, seed=42):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \n",
    "        self.reset_graph()\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "        \n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "        \n",
    "        dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "\n",
    "        with tf.name_scope('dnn'):\n",
    "            hidden1 = dense_layer(X, self.n_neuron, name='hidden1')\n",
    "            hidden2 = dense_layer(hidden1, self.n_neuron, name='hidden2')\n",
    "            hidden3 = dense_layer(hidden2, self.n_neuron, name='hidden3')\n",
    "            hidden4 = dense_layer(hidden3, self.n_neuron, name='hidden4')\n",
    "            hidden5 = dense_layer(hidden4, self.n_neuron, name='hidden5')\n",
    "            logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
    "            \n",
    "        with tf.name_scope('softmax'):\n",
    "            y_proba = tf.nn.softmax(logits, axis=1, name='y_proba')\n",
    "    \n",
    "        with tf.name_scope('loss'):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name='loss')\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            \n",
    "        with tf.name_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            \n",
    "        self._training_op = training_op\n",
    "        self._accuracy = accuracy\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._logits = logits\n",
    "        self._y_proba = y_proba\n",
    "        \n",
    "    def shuffle_split(self, X, y, n_batches):\n",
    "        np.random.seed(seed=42)\n",
    "        rnd_idx = np.random.permutation(len(X))\n",
    "        for i_idx in np.array_split(rnd_idx, n_batches):\n",
    "            X_batch = X[i_idx]\n",
    "            y_batch = y[i_idx]\n",
    "            yield X_batch, y_batch\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=5):\n",
    "        self.n_batches = len(X) // self.batch_size\n",
    "        \n",
    "        n_inputs = X.shape[1]\n",
    "        self.n_outputs = len(np.unique(y))\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        self._build_graph(n_inputs, self.n_outputs)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        self._session = tf.Session()\n",
    "        with self._session.as_default() as sess:\n",
    "            init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                for X_batch, y_batch in self.shuffle_split(X, y, self.n_batches):\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    sess.run([self._training_op], feed_dict=feed_dict)\n",
    "                accuracy_train = self._accuracy.eval(feed_dict=feed_dict)\n",
    "                print(epoch, 'accuracy_train:', accuracy_train)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        with self._session.as_default() as sess:\n",
    "            y_proba = sess.run([self._y_proba], feed_dict={self._X: X})\n",
    "            return np.array(y_proba).reshape((len(X), self.n_outputs))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
