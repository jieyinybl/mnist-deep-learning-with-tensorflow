# MNIST Deep Learning with TensorFlow

The notebook works on exercises of *Hands-On Machine Learning with Scikit-Learn and TensorFlow* from Aurélien Géron.

The exercises come from Chapter 11: Training Deep Neural Nets. The complete exercises include:

- 8.1 Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.

- 8.2 Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.

- 8.3 Tune the hyperparameters using cross-validation and see what precision you can achieve.

- 8.4 Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?

- 8.5 Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?“

